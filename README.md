# SHL Gen AI Assessment Recommender

This repository contains the production-oriented pipeline that powers SHL's
Gen AI assessment recommender. The service ingests free-form job descriptions
and returns a curated shortlist of SHL assessments while respecting policy
limits (minimum, maximum, and preferred counts) and balancing knowledge,
skills, aptitude, and behavioural content.

## Architecture Overview

The end-to-end flow is orchestrated from `src/api.py` and is built from the
following modular stages:

1. **Retrieval** (`src/retrieval.py`)
   * BM25 lexical retrieval and dense vector retrieval are fused to produce an
     initial candidate pool.
   * Retrieval depth and weighting are governed by configuration in
     `src/config.py`.

2. **Reranking** (`src/rerank.py`)
   * A cross-encoder reranker (BAAI/bge-reranker-base) scores retrieved items
     against the query for semantic relevance.

3. **Intent & Seed Expansion** (`src/intent_utils.py`, `src/constants.py`)
   * Natural-language heuristics map queries onto intent keys which unlock
     curated seed lists (`RETRIEVAL_BOOST_SEEDS`, `EXPANSION_LIBRARY`).
   * Canonical intent keys use `INTENT_KEY_ALIASES` to avoid duplicate seed
     expansion.

4. **Heuristic Post-processing** (`src/api.py` helpers)
   * Canonical families are pinned, must-include sets injected, and duration
     hints applied.
   * Domain vetoes, duplicate suppression, and stack-aware boosts maintain
     relevance and diversity for mixed-intent briefs.

5. **Diversification & Allocation**
   * Maximum Marginal Relevance (`src/mmr.py`) diversifies the pool using the
     learned item embeddings.
   * `src/balance.py` allocates final results across Knowledge/Skills and
     Behaviour/Aptitude categories while respecting soft targets.

6. **Mapping** (`src/mapping.py`)
   * Ranked items are converted into the public API schema returned by the
     FastAPI `/recommend` endpoint.

## Key Modules

| Module | Responsibility |
| ------ | -------------- |
| `src/api.py` | FastAPI application, pipeline orchestration, heuristics, and CLI glue. |
| `src/config.py` | Central configuration: retrieval weights, model names, seed lists, aliases, and trigger phrases. |
| `src/constants.py` | Shared keyword vocabularies used across heuristics and intent detection. |
| `src/intent_utils.py` | Query intent parsing, generic developer detection, and seed-key clamping. |
| `src/retrieval.py` | BM25 + dense retrieval fusion. |
| `src/rerank.py` | Cross-encoder reranking utilities. |
| `src/mmr.py` | MMR diversification using pre-computed item embeddings. |
| `src/balance.py` | Category balancing and allocation logic. |
| `src/mapping.py` | Mapping catalog rows to API response models. |

## Local Development

### Environment Setup

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

The retrieval and rerank stages expect the catalog snapshot and embedding
artifacts generated by `src/catalog_build.py` and `src/embed_index.py`.

### Running the API

```bash
uvicorn src.api:app --reload
```

The primary endpoint is `POST /recommend` which accepts a JSON body:

```json
{
  "query": "Looking to hire mid-level professionals who are proficient in Python, SQL and Java Script. Need an assessment package that can test all skills with max duration of 60 minutes."
}
```

### Smoke Testing

Two lightweight checks help validate changes:

1. **Python bytecode compilation**
   ```bash
   python -m compileall src
   ```
2. **Pipeline smoke test**
   ```python
   python - <<'PY'
   from src.catalog_build import load_catalog_snapshot
   from src.api import run_full_pipeline

   df = load_catalog_snapshot().set_index("item_id", drop=False)
   response = run_full_pipeline(
       "Looking to hire mid-level professionals who are proficient in Python, SQL and Java Script. Need an assessment package that can test all skills with max duration of 60 minutes.",
       df,
       intent_model=None,
   )
   print(len(response.recommended_assessments))
   PY
   ```

The smoke test should return a non-zero count with stack-aligned assessments
(e.g., Python, SQL, JavaScript) under the duration constraint.

### CLI Evaluation

The CLI driver (`src/cli.py`) can batch evaluate queries defined in
`data/gen_ai_test.xlsx` and emits `artifacts/test_predictions.csv`:

```bash
python -m src.cli --mode test
```

## Repository Conventions

* Keep heuristic edits surgical; the pipeline already performs well on core
  benchmark queries (AI engineer, Product Manager, Finance & Ops Analyst,
  Content Writer, Customer Support, Analyst Cognitive + Personality).
* Respect the result policy defined in `src/config.py` (`RESULT_MIN`,
  `RESULT_MAX`, `RESULT_DEFAULT_TARGET`).
* Avoid altering FastAPI request/response models or the public surface area of
  `src/api.py`.
* Use the shared constants in `src/constants.py` instead of duplicating keyword
  lists across modules.

## Data Privacy

The catalog snapshot and embedding artifacts are proprietary to SHL. Ensure
that derivative artifacts are handled in accordance with SHL data governance
policies.

## Support

For questions or issue triage:

- Create a GitHub issue with reproduction details and the query payload.
- Escalate urgent production incidents through the SHL Gen AI on-call rotation.

